{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import googlemaps\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Place your google API key (with full Maps API access) in the main project directory.~~\n",
    "\n",
    "Place your mapillary API key in a text doc \"mapillary_client_token.txt\" in the project directory.\n",
    "https://www.mapillary.com/dashboard/developers\n",
    "Just make a new registration and it should automatically give you the client token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading api key + cities to pull data for\n",
    "parent_folder = os.path.dirname(os.path.dirname(__name__))\n",
    "#google_api_key = open(parent_folder + \"google_maps_api_key.txt\", \"r\").read()\n",
    "mapillary_client_id = open(parent_folder + \"mapillary_client_token.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openstreetmap API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API rate limit handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### API rate limit stuff\n",
    "import time\n",
    "import requests\n",
    "def backoff(retries):\n",
    "    # To avoid API limit\n",
    "    time.sleep(2 ** retries)\n",
    "\n",
    "rate_limits = {\n",
    "    \"entity\": 60000,\n",
    "    \"search\": 10000,\n",
    "    \"tiles\": 50000  # Note: This is per day\n",
    "}\n",
    "\n",
    "requests_made = {\n",
    "    \"entity\": 0,\n",
    "    \"search\": 0,\n",
    "    \"tiles\": 0\n",
    "}\n",
    "\n",
    "def request_with_rate_limiting(url, headers=None, params=None, type=\"entity\"):\n",
    "    # Should exponentially slow down once we hit rate limit.\n",
    "    global requests_made\n",
    "    retries = 0\n",
    "    while True:\n",
    "        if requests_made[type] < rate_limits[type]:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            requests_made[type] += 1\n",
    "            if response.status_code == 429:  # Rate limit exceeded error\n",
    "                print(\"Rate limit exceeded, retrying...\")\n",
    "                retries += 1\n",
    "                backoff(retries)\n",
    "            else:\n",
    "                return response\n",
    "        else:\n",
    "            print(f\"Approaching rate limit for {type}. Waiting...\")\n",
    "            time.sleep(60)  # Wait for 1 minute before retrying\n",
    "            requests_made[type] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import mercantile\n",
    "from vt2geojson.tools import vt_bytes_to_geojson\n",
    "import time\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyproj import Transformer \n",
    "\n",
    "country_list = pd.read_csv('./Data/country_list.csv')\n",
    "european = country_list['country'].unique()\n",
    "\n",
    "# World Mollweide (EPSG: 54009) \n",
    "urban_areas = gpd.read_file('./Data/urban/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg')\n",
    "\n",
    "# GPD world object #'epsg:4326', (lon, lat)\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "europe = world[world['name'].isin(european)]\n",
    "urban_areas = urban_areas.to_crs(\"EPSG:4326\") # Converting to 4326\n",
    "# buffering\n",
    "europe['geometry'] = europe['geometry'].apply(lambda x: x if x.is_valid else x.buffer(0))\n",
    "urban_areas['geometry'] = urban_areas['geometry'].apply(lambda x: x if x.is_valid else x.buffer(0))\n",
    "### Filtering to only europe\n",
    "urban_areas_in_europe = gpd.sjoin(urban_areas, europe, how=\"inner\", op='intersects')\n",
    "\n",
    "dataset_file = \"./Data/mapillary/dataset_mapillary.csv\"\n",
    "\n",
    "# Check if the dataset file exists\n",
    "if os.path.exists(dataset_file):\n",
    "    # If the file exists, load the dataset from the file\n",
    "    dataset = pd.read_csv(dataset_file, index_col=0)\n",
    "    n = len(dataset)\n",
    "    print(f\"Resuming from index {n}\")\n",
    "else:\n",
    "    # If the file doesn't exist, create a new dataset DataFrame\n",
    "    dataset = pd.DataFrame(columns=[\"lat\", \"lon\", \"width\", \"height\"])\n",
    "    n = 0\n",
    "\n",
    "tile_coverage = 'mly1_public'\n",
    "tile_layer = \"image\"\n",
    "N = 2500\n",
    "start_time = time.time()\n",
    "\n",
    "### Search loop\n",
    "while n < N:\n",
    "\n",
    "    ### Should be in 4326 format\n",
    "    urban_polygon = urban_areas_in_europe.sample(1).iloc[0].geometry\n",
    "\n",
    "    ### Returned as (long, lat, long, lat)\n",
    "    west, south, east, north = urban_polygon.bounds\n",
    "\n",
    "    #print(f\"West, south, east, north {west} {south} {east} {north}\")\n",
    "\n",
    "    zoom = 14\n",
    "    tiles = list(mercantile.tiles(west, south, east, north, zoom))\n",
    "\n",
    "    max_img = False\n",
    "    for tidx, tile in enumerate(tiles):\n",
    "\n",
    "        if max_img:\n",
    "            break\n",
    "\n",
    "        tile_url = f'https://tiles.mapillary.com/maps/vtp/{tile_coverage}/2/{tile.z}/{tile.x}/{tile.y}?access_token={mapillary_client_id}'\n",
    "        response = request_with_rate_limiting(tile_url, type=\"tiles\")\n",
    "        #response = requests.get(tile_url)\n",
    "        data = vt_bytes_to_geojson(response.content, tile.x, tile.y, tile.z, layer=tile_layer)\n",
    "\n",
    "        ### If there are no features, this skips\n",
    "        for idx, feature in enumerate(data['features']):\n",
    "            # Get lng,lat of each feature\n",
    "            lng = feature['geometry']['coordinates'][0]\n",
    "            lat = feature['geometry']['coordinates'][1]\n",
    "            \n",
    "            ### Checking what country we're in.\n",
    "            point = gpd.GeoDataFrame([{'geometry': Point(lng, lat)}], crs='EPSG:4326')\n",
    "            point_in_country = gpd.sjoin(point, world, how=\"inner\", op='intersects')\n",
    "            country_name = point_in_country.iloc[0]['name']  # Assuming 'name' is the country name column in world\n",
    "\n",
    "            if idx == 0:\n",
    "                print(f\"Tile {tidx} is in {country_name}\")    \n",
    "\n",
    "            sequence_id = feature['properties']['sequence_id']\n",
    "            _folder = os.path.join(parent_folder, \"Data\", \"mapillary\")\n",
    "\n",
    "            # Request the URL of each image\n",
    "            image_id = feature['properties']['id']\n",
    "            url = f'https://graph.mapillary.com/{image_id}?fields=thumb_2048_url,width,height'\n",
    "            headers = {'Authorization': f'OAuth {mapillary_client_id}'}\n",
    "            #r = requests.get(url, headers=headers)\n",
    "            r = request_with_rate_limiting(url, headers=headers, type=\"search\")\n",
    "            data = r.json()\n",
    "            \n",
    "            try:\n",
    "                image_url = data['thumb_2048_url']\n",
    "            except:\n",
    "                print(f\"KeyError: 'thumb_2048_url' \\nNot sure why some images don't have the 2048 url, but we can just skip.\")\n",
    "                continue\n",
    "            \n",
    "            image_width = data['width']\n",
    "            image_height = data['height']\n",
    "\n",
    "            # Save each image with ID as filename to directory by sequence ID\n",
    "            image_path = os.path.join(_folder, f\"{n}.jpg\")\n",
    "            with open(image_path, 'wb') as handler:\n",
    "                image_data = requests.get(image_url, stream=True).content\n",
    "                handler.write(image_data)\n",
    "            dataset.loc[n] = [lat, lng, image_width, image_height]\n",
    "            print(f\"Succesful image found for index {n} out of {N}\")\n",
    "            n += 1\n",
    "\n",
    "            ### Periodically saving\n",
    "            if n % 50 == 0:\n",
    "                print(f\"Saved dataset at n = {n}.\")\n",
    "                dataset.to_csv(dataset_file)\n",
    "\n",
    "            # We don't want, e.g. 4000 images from Paris to clog the dataset.\n",
    "            # So, break back to next tile, exit the tile loop, pick a new polygon.\n",
    "            if idx > 10:\n",
    "                print(f\"Over 10 images found for this tile. Moving onto a new polygon.\")\n",
    "                max_img = True\n",
    "                break;\n",
    "\n",
    "dataset.to_csv(dataset_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For checking if your API key is set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mercantile, mapbox_vector_tile, requests, json, os\n",
    "from vt2geojson.tools import vt_bytes_to_geojson\n",
    "\n",
    "def download_mapillary_images(west, south, east, north, mapillary_client_id, parent_folder, image_num):\n",
    "    # Vector tile endpoint\n",
    "    tile_coverage = 'mly1_public'\n",
    "    tile_layer = \"image\"\n",
    "\n",
    "    # Get the list of tiles with x and y coordinates which intersect our bounding box\n",
    "    # MUST be at zoom level 14 where the data is available, other zooms currently not supported\n",
    "    tiles = list(mercantile.tiles(west, south, east, north, 14))\n",
    "\n",
    "    # Loop through list of tiles to get tile z/x/y to plug in to Mapillary endpoints and make request\n",
    "    for tile in tiles:\n",
    "        tile_url = f'https://tiles.mapillary.com/maps/vtp/{tile_coverage}/2/{tile.z}/{tile.x}/{tile.y}?access_token={mapillary_client_id}'\n",
    "        response = requests.get(tile_url)\n",
    "        data = vt_bytes_to_geojson(response.content, tile.x, tile.y, tile.z, layer=tile_layer)\n",
    "        print(response)\n",
    "        # Process each feature in the tile\n",
    "        for idx, feature in enumerate(data['features']):\n",
    "            print(f\"On feature {idx}\")\n",
    "            # Get lng,lat of each feature\n",
    "            lng = feature['geometry']['coordinates'][0]\n",
    "            lat = feature['geometry']['coordinates'][1]\n",
    "\n",
    "            # Ensure feature falls inside bounding box since tiles can extend beyond\n",
    "            if lng > west and lng < east and lat > south and lat < north:\n",
    "                # Create a folder for each unique sequence ID to group images by sequence\n",
    "                sequence_id = feature['properties']['sequence_id']\n",
    "                sequence_folder = os.path.join(parent_folder, \"Data\", \"mapillary\", sequence_id)\n",
    "                os.makedirs(sequence_folder, exist_ok=True)\n",
    "\n",
    "                # Request the URL of each image\n",
    "                image_id = feature['properties']['id']\n",
    "                url = f'https://graph.mapillary.com/{image_id}?fields=thumb_2048_url'\n",
    "                headers = {'Authorization': f'OAuth {mapillary_client_id}'}\n",
    "                r = requests.get(url, headers=headers)\n",
    "                data = r.json()\n",
    "                image_url = data['thumb_2048_url']\n",
    "\n",
    "                # Save each image with ID as filename to directory by sequence ID\n",
    "                image_path = os.path.join(sequence_folder, f\"{image_id}.jpg\")\n",
    "                with open(image_path, 'wb') as handler:\n",
    "                    image_data = requests.get(image_url, stream=True).content\n",
    "                    handler.write(image_data)\n",
    "\n",
    "# Example usage\n",
    "west, south, east, north = [-80.13423442840576, 25.77376933762778, -80.1264238357544, 25.788608487732198]\n",
    "\n",
    "download_mapillary_images(west, south, east, north, mapillary_client_id, parent_folder, image_num=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old Google Maps API code (apparently data scraping is against their TOS? Who would've thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Given a coord, randomly select a nearby coord to grab. This is best for picking within cities.\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import shapely.geometry as geom\n",
    "def city_coords(countries, world, urban_areas):\n",
    "    searching = True\n",
    "    while searching:\n",
    "        lat = random.uniform(-90, 90)\n",
    "        lon = random.uniform(-180, 180)\n",
    "\n",
    "        # Create a shapely Point from the latitude and longitude\n",
    "        point = geom.Point(lon, lat)\n",
    "\n",
    "        # Create a GeoDataFrame with the random point\n",
    "        point_gdf = gpd.GeoDataFrame(geometry=[point], crs=\"EPSG:4326\")\n",
    "        ### Casting to 50049\n",
    "        point_gdf = point_gdf.to_crs(urban_areas.crs)\n",
    "\n",
    "        # Perform a spatial join to find the urban area that contains the point\n",
    "        result_c = gpd.sjoin(point_gdf, world.to_crs(urban_areas.crs), op='within')\n",
    "        \n",
    "        # Check if the point is within a target country\n",
    "        if not result_c.empty:\n",
    "            country = result_c.iloc[0]['name']\n",
    "            if country in countries:\n",
    "                # Perform a spatial join to find the urban area that contains the point\n",
    "                result_u = gpd.sjoin(point_gdf, urban_areas, op='within')\n",
    "                \n",
    "                # Check if the point is within an urban area\n",
    "                if not result_u.empty:\n",
    "                    urban_area = result_u.iloc[0]['eFUA_name']\n",
    "                    #print(f\"The point ({lat}, {lon}) is in the urban area: {urban_area}\")\n",
    "                    return (lat, lon, country)\n",
    "                # else:\n",
    "                #     print(f\"The point ({lat}, {lon}) is in {country} but not in an urban area. Trying again.\")\n",
    "            # else:\n",
    "            #     print(f\"Country is {country}. Trying again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting lat-long to street address\n",
    "\n",
    "def get_address(gmaps, coords):\n",
    "    try:\n",
    "        # Look up an address with reverse geocoding\n",
    "        reverse_geocode_result = gmaps.reverse_geocode(coords)\n",
    "        \n",
    "        if reverse_geocode_result:\n",
    "            geocode_components = reverse_geocode_result[0]\n",
    "            coms = geocode_components['address_components']\n",
    "            \n",
    "            street_number = \"\"\n",
    "            street_name = \"\"\n",
    "            city = \"\"\n",
    "            state = \"\"\n",
    "            country = \"\"\n",
    "            postal_code = \"\"\n",
    "            \n",
    "            for component in coms:\n",
    "                if 'street_number' in component['types']:\n",
    "                    street_number = component['long_name']\n",
    "                elif 'route' in component['types']:\n",
    "                    street_name = component['long_name']\n",
    "                elif 'locality' in component['types']:\n",
    "                    city = component['long_name']\n",
    "                elif 'administrative_area_level_1' in component['types']:\n",
    "                    state = component['long_name']\n",
    "                elif 'country' in component['types']:\n",
    "                    country = component['long_name']\n",
    "                elif 'postal_code' in component['types']:\n",
    "                    postal_code = component['long_name']\n",
    "            \n",
    "            address = f\"{street_number} {street_name}, {city}, {state}, {country} {postal_code}\"\n",
    "            return city, address\n",
    "        else:\n",
    "            #print(\"No address found for the given coordinates.\")\n",
    "            return None, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Given a coord, randomly select a nearby coord to grab. This is best for picking randomly inside countries, allowing for rural picks.\n",
    "\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import shapely.geometry as geom\n",
    "\n",
    "#args: countries (list of countries we want), world (GPD object)\n",
    "def gen_coords(countries, world):\n",
    "    searching = True\n",
    "    while searching:\n",
    "        lat = random.uniform(-90, 90) ### Randomly selecting a latlong\n",
    "        lon = random.uniform(-180, 180)\n",
    "        ### Create a shapely Point from the latitude and longitude\n",
    "        point = geom.Point(lon, lat)\n",
    "\n",
    "        ### Create a GeoDataFrame with the random point\n",
    "        point_gdf = gpd.GeoDataFrame(geometry=[point], crs=\"EPSG:4326\")\n",
    "\n",
    "        ### Perform a spatial join to find the country that contains the point\n",
    "        result = gpd.sjoin(point_gdf, world, op='within')\n",
    "\n",
    "        if not result.empty:\n",
    "            country = result.iloc[0]['name']\n",
    "            if country in countries:\n",
    "                #print(f\"The point ({lat}, {lon}) is in {country}, which is one of the target countries.\")\n",
    "                return (lat, lon, country)\n",
    "            # else:\n",
    "            #     print(f\"Country is {country}. Trying again\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gmaps object\n",
    "gmaps = googlemaps.Client(key=google_api_key)\n",
    "\n",
    "### list of target countries\n",
    "country_list = pd.read_csv('./Data/country_list.csv')\n",
    "european = country_list['country'].unique()\n",
    "\n",
    "### GPD world object\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) ### Creating here so it doesn't need to be re-created every execution\n",
    "\n",
    "### urban areas databse (GHS)\n",
    "urban_areas = gpd.read_file('./Data/urban/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg') ### A list of polygon-bounded coordinates\n",
    "\n",
    "### Dataset file for the pipeline\n",
    "dataset_file = \"./Data/gmaps/dataset.csv\"\n",
    "\n",
    "# Check if the dataset file exists\n",
    "if os.path.exists(dataset_file):\n",
    "    # If the file exists, load the dataset from the file\n",
    "    dataset = pd.read_csv(dataset_file, index_col=0)\n",
    "    start_index = len(dataset)\n",
    "    print(f\"Resuming from index {start_index}\")\n",
    "else:\n",
    "    # If the file doesn't exist, create a new dataset DataFrame\n",
    "    dataset = pd.DataFrame(columns=[\"lat\", \"lon\", \"country\", \"address\"])\n",
    "    start_index = 0\n",
    "\n",
    "### Dataset to be saved for identification of each image w/ latlong \n",
    "#dataset = pd.DataFrame(columns=[\"lat\", \"lon\", \"country\", \"address\"])\n",
    "\n",
    "N = 2500\n",
    "for n in range(start_index, start_index + N):\n",
    "    ### Selecting 33% rural (randomly sampled within the country) and 66% urban (bounded by urban_areas)\n",
    "    searching = True\n",
    "    while searching:\n",
    "        if n % 3:\n",
    "            lat, lon, country = city_coords(european, world, urban_areas)\n",
    "        else:\n",
    "            lat, lon, country = gen_coords(european, world)\n",
    "        city, address = get_address(gmaps, (lat, lon))\n",
    "        \n",
    "        if city is None or address is None:\n",
    "            continue\n",
    "\n",
    "        ### Checking if the rounded coordinates already exist in the DataFrame. Surprising amount of duplicates.\n",
    "        lat_rounded = round(lat, 6)\n",
    "        lon_rounded = round(lon, 6)\n",
    "        if len(dataset[(dataset['lat'].round(6) == lat_rounded) & (dataset['lon'].round(6) == lon_rounded)]) > 0:\n",
    "            print(f\"Coordinates ({lat_rounded}, {lon_rounded}) already exist in the dataset. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        ### Russia is gigantic. Skipping ~2/3 of all Russia hits. \n",
    "        if n % 3 and country == \"Russia\":\n",
    "            continue\n",
    "\n",
    "        # print(f\"Address found for image {n}!\")\n",
    "\n",
    "        ### If we've found an address that exists in the database, we can try querying the API for a streetview image.\n",
    "        pic_base = 'https://maps.googleapis.com/maps/api/streetview?'\n",
    "\n",
    "        ### define the params for the picture request\n",
    "        pic_params = {'key': google_api_key,\n",
    "                'location': address,\n",
    "                'size': \"512x512\"}\n",
    "        \n",
    "        ### Requesting data\n",
    "        pic_response = requests.get(pic_base, params=pic_params)\n",
    "        # print(f\"API Response:\")\n",
    "        # print(f\"Status Code: {pic_response.status_code}\")\n",
    "        # print(f\"Headers: {pic_response.headers}\")\n",
    "        # print(f\"Content Type: {pic_response.headers['Content-Type']}\")\n",
    "        # print(f\"Content Length: {len(pic_response.content)} bytes\")\n",
    "\n",
    "        if pic_response.status_code == 200:\n",
    "            ### If the image cannot be retrieved, gmaps will still return a blank image. The image size is typically ~6kB-9kB. \n",
    "            if len(pic_response.content) > 10000:\n",
    "                image_name = f\"{n}.jpg\"\n",
    "                image_path = parent_folder + \"Data/gmaps/\" + image_name\n",
    "                \n",
    "                with open(image_path, \"wb\") as file:\n",
    "                    file.write(pic_response.content)\n",
    "                dataset.loc[len(dataset)] = [lat, lon, country, address]\n",
    "                searching = False\n",
    "                print(f\"Image found for image {n} in {country}.\")\n",
    "            # else:\n",
    "            #     print(f\"Image size is too small, {len(pic_response.content)} bytes - no image exists.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve street view image for address: {address}\")\n",
    "\n",
    "    if n % 100 == 0:\n",
    "        print(f\"Saved dataset at n = {n}.\")\n",
    "        dataset.to_csv(dataset_file)\n",
    "\n",
    "dataset.to_csv(dataset_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
