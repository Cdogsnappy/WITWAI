{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import easyocr\n",
    "from collections import Counter\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/dataset/european_images.csv', index_col=0)\n",
    "df['img_path'] = [f\"./Data/dataset/{index}.png\" for index in df.index]\n",
    "df['text'] =  pd.Series(dtype='object')\n",
    "df['confidence'] =  pd.Series(dtype='object')\n",
    "df['bbox'] =  pd.Series(dtype='object')\n",
    "df['lang'] = pd.Series(None)\n",
    "df['best_confidence'] = pd.Series(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to determine the best OCR model for this.\n",
    "\n",
    "Candidates:\n",
    "    easyocr (built in, uses pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Text Recognition\n",
    "1. It's important to note that we're looking at a plethora of European languages, which will employ various modifications to the latin script, cyrillic, etc. \n",
    "2. As such, multiple OCR readers can be employed. We can run each reader on an image, extract the confidence and various pieces of text above some threshold, and then input the collected data to our classifier.\n",
    "3. We could also use a reader with multiple languages in the list. This will remove the ability to handle different languages differently, but that may be better to avoid data leakage?\n",
    "4. It may be necessary to prune the 'Google' labels from the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Text Recognition: Language definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jaided.ai/easyocr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ocr_reader():\n",
    "    def __init__(self, langs, recog_network = \"standard\"):\n",
    "        self.langs = langs\n",
    "        self.reader = easyocr.Reader(langs, detect_network = 'dbnet18', recog_network=recog_network)\n",
    "    \n",
    "    def read(self, df):\n",
    "        min_length = 3\n",
    "        self.df_results = df.copy()\n",
    "        \n",
    "        for index, row in self.df_results.iterrows():\n",
    "            \n",
    "            results = self.OCR_extraction(row['img_path'], self.reader)\n",
    "            \n",
    "            bbox_ = []\n",
    "            text = []\n",
    "            confidence = []\n",
    "            for result in results:\n",
    "                if len(result[1]) >= min_length:\n",
    "                    bbox_.append(result[0])\n",
    "                    text.append(result[1])\n",
    "                    confidence.append(result[2])\n",
    "                \n",
    "            self.df_results.at[index, \"bbox\"] = bbox_\n",
    "            self.df_results.at[index, \"text\"] = text\n",
    "            self.df_results.at[index, \"confidence\"] = confidence\n",
    "            if index == 500:\n",
    "                print(f\"Here! {text}\")\n",
    "                print(f\"Lang detection gives {self.detect_Lang(row['img_path'])}\")\n",
    "        return self.df_results\n",
    "    \n",
    "    def OCR_extraction(self, path, reader):\n",
    "        results =  reader.readtext(path)\n",
    "        return results\n",
    "    \n",
    "    def detect_Lang(self, image):\n",
    "\n",
    "        language_result = self.reader.readtext(image, paragraph = True) #, text_threshold=0.15, low_text=0.3, mag_ratio=1.2)\n",
    "        if language_result:\n",
    "            return language_result\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {\n",
    "    \"be\": \"be\",\n",
    "    \"bg\": \"bg\",\n",
    "    \"cs\": \"cs\",\n",
    "    \"cy\": \"cy\",\n",
    "    \"da\": \"da\",\n",
    "    \"de\": \"de\",\n",
    "    \"en\": \"en\",\n",
    "    \"es\": \"es\",\n",
    "    \"et\": \"et\",\n",
    "    \"fr\": \"fr\",\n",
    "    \"ga\": \"ga\",\n",
    "    \"hr\": \"hr\",\n",
    "    \"hu\": \"hu\",\n",
    "    \"is\": \"is\",\n",
    "    \"it\": \"it\",\n",
    "    \"la\": \"la\",\n",
    "    \"lt\": \"lt\",\n",
    "    \"lv\": \"lv\",\n",
    "    \"mt\": \"mt\",\n",
    "    \"nl\": \"nl\",\n",
    "    \"no\": \"no\",\n",
    "    \"pl\": \"pl\",\n",
    "    \"pt\": \"pt\",\n",
    "    \"ro\": \"ro\",\n",
    "    \"ru\": \"ru\",\n",
    "    \"rs_latin\": \"rs\",\n",
    "    \"rs_cyrillic\": \"rc\",\n",
    "    \"sk\": \"sk\",\n",
    "    \"sl\": \"sl\",\n",
    "    \"sq\": \"sq\",\n",
    "    \"sv\": \"sv\",\n",
    "    \"tr\": \"tr\",\n",
    "    \"uk\": \"uk\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_languages = [\n",
    "    \"cs\", \"cy\", \"da\", \"de\", \"en\", \"es\", \"et\", \"fr\", \"ga\", \"hr\", \"hu\", \"is\", \"it\", \"la\", \"lt\", \"lv\",\n",
    "    \"mt\", \"nl\", \"no\", \"pl\", \"pt\", \"ro\", \"rs_latin\", \"sk\", \"sl\", \"sq\", \"sv\", \"tr\"\n",
    "]\n",
    "\n",
    "cyrillic_languages = [\n",
    "    \"ru\",\"rs_cyrillic\",\"be\",\"bg\",\"uk\",\"en\"\n",
    "]\n",
    "\n",
    "Readers = {\n",
    "\"en\": ocr_reader([\"en\"]),\n",
    "\"latin\": ocr_reader(latin_languages),\n",
    "\"cyrillic\": ocr_reader(cyrillic_languages, recog_network=\"cyrillic_g2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either only store text above a certain threshold for all, or only store text above a certain threshold for each language.\n",
    "On CPU, roughly 3s to initialize the OCR reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Google watermarks from detected text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text for each image for each reader, then using manual pruning to determine which labels to drop for each language.\n",
    "These can then be associated with the correct language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the english reader, identify matching watermark text, determine the bounding box, and mask the image for that watermark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OCR_extraction(path, reader):\n",
    "    results =  reader.readtext(path)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.makedirs(\"./Data/masked_for_ocr/\", exist_ok=True)\n",
    "\n",
    "reader_en = Readers[\"en\"].reader\n",
    "\n",
    "watermarks = [\"gccole\", \"cgoogle\", \"cgccgle\", \"cgcogle\", \"cgocgle\", \"google\", \"gccgle\", \"gcogle\", \"gocgle\", \"gocnle\", \"gcogie\", \"oogle\"]\n",
    "\n",
    "### For each image, generate a mask (initially blank). Use this to mask results for the other readers.\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ### Generating the mask with the english reader, filtering out the sections of the image containing the \"Google\" watermark.\n",
    "    image = cv2.imread(row['img_path'])\n",
    "    mask = np.ones(image.shape[:2], dtype=np.uint8) * 255\n",
    "    results = OCR_extraction(row['img_path'], reader_en)\n",
    "    \n",
    "    bbox_ = []\n",
    "    text = []\n",
    "    confidence = []\n",
    "    for result in results:\n",
    "        bbox_.append(result[0])\n",
    "        text.append(result[1])\n",
    "        confidence.append(result[2])\n",
    "    \n",
    "    for idx, t in enumerate(text):\n",
    "        ### Filtereing text matching the filter, or text with less than min_length characters (meaningless)\n",
    "        min_length = 3\n",
    "        if any(watermark.upper() in t.upper() for watermark in watermarks) or len(t) <= min_length:\n",
    "            corners = bbox_[idx]\n",
    "            x0, y0 = np.array(corners[0], dtype=np.int32) ### Top left corner\n",
    "            x1, y1 = np.array(corners[2], dtype=np.int32) ### Bottom right corner\n",
    "            cv2.rectangle(mask, (x0, y0), (x1, y1), 0, -1)\n",
    "        else:\n",
    "            print(f\"Text {t.upper()} passed the filter for img index {index} and confidence {confidence[idx]}\")\n",
    "    \n",
    "    ### We now have a masked image instead of the original image. Other lang readers can now use this image.\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    ### Store the images in a new directory (masked_for_ocr/) so that I only have to do this once\n",
    "    cv2.imwrite(f\"./Data/masked_for_ocr/{index}.png\", masked_image)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df = pd.read_csv('./Data/dataset/european_images.csv', index_col=0)\n",
    "masked_df['img_path'] = [f\"./Data/masked_for_ocr/{index}.png\" for index in df.index]\n",
    "masked_df['text'] =  pd.Series(dtype='object')\n",
    "masked_df['confidence'] =  pd.Series(dtype='object')\n",
    "masked_df['bbox'] =  pd.Series(dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, with the text extracted from the masked images, we can set confidence thresholds, and only keep results above specific thresholds.\n",
    "\n",
    "##### Identify the language with the highest confidence, and use this as our guessed label\n",
    "##### Additionally, we can extract text and pass forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_df = Readers[\"latin\"].read(masked_df)\n",
    "cyrillic_df = Readers[\"cyrillic\"].read(masked_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the text for each image is passed through an OCR model for a) Latin and b) Cyrillic text. \n",
    "\n",
    "It may be better to take the best guesses and input them into an LLM instead of using OCR, but this would be computationally more expensive, since we're already using easyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.3\n",
    "### For each row, iterate through the language DF and identify the highest confidence text\n",
    "### If no text was found, \"best_lang\" = None and thus df['lang'] = None\n",
    "index = None\n",
    "\n",
    "for idx, _ in df.iterrows():\n",
    "    print(f\"On row {idx}\")\n",
    "    best_confidence = 0.0\n",
    "    best_lang = None\n",
    "    best_text = \"\"\n",
    "    conf_c = 0\n",
    "    conf_l = 0\n",
    "    \n",
    "    \n",
    "    latin_res = latin_df.loc[idx]\n",
    "    cyril_res = cyrillic_df.loc[idx]\n",
    "\n",
    "    if latin_res['text'] != [[]] and latin_res['text'] != [] and latin_res['text'] is not None:\n",
    "        print(f\"Woah we found text! {latin_res['text']}\")\n",
    "        for indx, text_l in enumerate(latin_res['text']):\n",
    "            \n",
    "            c = latin_res['confidence'][indx]\n",
    "            if c < THRESHOLD:\n",
    "                #print(c)\n",
    "                continue\n",
    "\n",
    "            \n",
    "            ### Only try identifying text if c is greater than the threshold.\n",
    "\n",
    "            #guess_ = Readers[\"latin\"].detect_Lang(masked_df.at[idx, 'img_path'])\n",
    "            guess_ = detect(text_l)\n",
    "            print(guess_)\n",
    "            print(f\"Image is {masked_df.at[idx,'img_path']}\")\n",
    "\n",
    "            if guess_ is not None and guess_!= [[]]:\n",
    "                text_out = guess_['text']\n",
    "\n",
    "    if cyril_res['text'] not in [[[]], [], None]:\n",
    "        for indx, text_c in enumerate(cyril_res['text']):\n",
    "\n",
    "            c = cyril_res['confidence'][indx]\n",
    "            if c < THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            guess_ = Readers[\"cyrillic\"].detect_Lang(masked_df.at[idx,'img_path'])\n",
    "\n",
    "            print(guess_)\n",
    "            print(f\"Image is {masked_df.at[idx,'img_path']}\")\n",
    "            if guess_ is not None and guess_!= [[]]:\n",
    "                text_out = guess_['text']\n",
    "\n",
    "\n",
    "\n",
    "    if best_confidence > THRESHOLD:\n",
    "    \n",
    "        df.at[idx, 'best_confidence'] = best_confidence\n",
    "        df.at[idx, 'lang'] = best_lang\n",
    "        print(f\"For image {idx}, lang found is {best_lang} with text {best_text}\")\n",
    "\n",
    "df.to_csv('./Outputs/OCR_results.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the lang data for OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./Outputs/languages.json', 'w') as json_file:\n",
    "    json.dump(languages, json_file, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
