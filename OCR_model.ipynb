{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/dataset/european_images.csv', index_col=0)\n",
    "df['img_path'] = [f\"./Data/dataset/{index}.png\" for index in df.index]\n",
    "df['text'] =  pd.Series(dtype='object')\n",
    "df['confidence'] =  pd.Series(dtype='object')\n",
    "df['bbox'] =  pd.Series(dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to determine the best OCR model for this.\n",
    "\n",
    "Candidates:\n",
    "    easyocr (built in, uses pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Text Recognition\n",
    "1. It's important to note that we're looking at a plethora of European languages, which will employ various modifications to the latin script, cyrillic, etc. \n",
    "2. As such, multiple OCR readers can be employed. We can run each reader on an image, extract the confidence and various pieces of text above some threshold, and then input the collected data to our classifier.\n",
    "3. We could also use a reader with multiple languages in the list. This will remove the ability to handle different languages differently, but that may be better to avoid data leakage?\n",
    "4. It may be necessary to prune the 'Google' labels from the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Text Recognition: Language definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jaided.ai/easyocr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ocr_reader():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.reader = easyocr.Reader([lang])\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        self.df_results = df.copy()\n",
    "        \n",
    "        for index, row in self.df.iterrows():\n",
    "            \n",
    "            results = self.OCR_extraction(row['img_path'], self.reader)\n",
    "            \n",
    "            bbox_ = []\n",
    "            text = []\n",
    "            confidence = []\n",
    "            for result in results:\n",
    "                bbox_.append(result[0])\n",
    "                text.append(result[1])\n",
    "                confidence.append(result[2])\n",
    "                \n",
    "            self.df_results.at[index, \"bbox\"] = bbox_\n",
    "            self.df_results.at[index, \"text\"] = text\n",
    "            self.df_results.at[index, \"confidence\"] = confidence\n",
    "        \n",
    "        return self.df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "languages = {\n",
    "    \"be\": \"be\",\n",
    "    \"bg\": \"bg\",\n",
    "    \"cs\": \"cs\",\n",
    "    \"cy\": \"cy\",\n",
    "    \"da\": \"da\",\n",
    "    \"de\": \"de\",\n",
    "    \"en\": \"en\",\n",
    "    \"es\": \"es\",\n",
    "    \"et\": \"et\",\n",
    "    \"fr\": \"fr\",\n",
    "    \"ga\": \"ga\",\n",
    "    \"hr\": \"hr\",\n",
    "    \"hu\": \"hu\",\n",
    "    \"is\": \"is\",\n",
    "    \"it\": \"it\",\n",
    "    \"la\": \"la\",\n",
    "    \"lt\": \"lt\",\n",
    "    \"lv\": \"lv\",\n",
    "    \"mt\": \"mt\",\n",
    "    \"nl\": \"nl\",\n",
    "    \"no\": \"no\",\n",
    "    \"pl\": \"pl\",\n",
    "    \"ro\": \"ro\",\n",
    "    \"ru\": \"ru\",\n",
    "    \"rs_latin\": \"rs\",\n",
    "    \"rs_cyrillic\": \"rc\",\n",
    "    \"sk\": \"sk\",\n",
    "    \"sl\": \"sl\",\n",
    "    \"sq\": \"sq\",\n",
    "    \"sv\": \"sv\",\n",
    "    \"uk\": \"uk\"\n",
    "}\n",
    "\n",
    "Readers = {}\n",
    "\n",
    "for key, lang in languages.items():\n",
    "    Readers[lang] = ocr_reader(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either only store text above a certain threshold for all, or only store text above a certain threshold for each language.\n",
    "On CPU, roughly 3s to initialize the OCR reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Google watermarks from detected text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text for each image for each reader, then using manual pruning to determine which labels to drop for each language.\n",
    "These can then be associated with the correct language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the english reader, identify matching watermark text, determine the bounding box, and mask the image for that watermark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OCR_extraction(path, reader):\n",
    "    results =  reader.readtext(path)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text SENUONIS passed the filter for img index 156 and confidence 0.00015776343750967432\n"
     ]
    }
   ],
   "source": [
    "reader_en = Readers[\"en\"].reader\n",
    "\n",
    "watermarks = [\"google\", \"gccgle\", \"gcogle\", \"gocgle\"]\n",
    "\n",
    "### For each image, generate a mask (initially blank). Use this to mask results for the other readers.\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ### Generating the mask with the english reader, filtering out the sections of the image containing the \"Google\" watermark.\n",
    "    image = cv2.imread(row['img_path'])\n",
    "    mask = np.ones(image.shape[:2], dtype=np.uint8) * 255\n",
    "    results = OCR_extraction(row['img_path'], reader_en)\n",
    "    \n",
    "    bbox_ = []\n",
    "    text = []\n",
    "    confidence = []\n",
    "    for result in results:\n",
    "        bbox_.append(result[0])\n",
    "        text.append(result[1])\n",
    "        confidence.append(result[2])\n",
    "    \n",
    "    for idx, t in enumerate(text):\n",
    "        ### Filtereing text matching the filter, or text with less than min_length characters (meaningless)\n",
    "        min_length = 3\n",
    "        if t.upper() in (watermark.upper() for watermark in watermarks) or len(t) <= min_length:\n",
    "            corners = bbox_[idx]\n",
    "            x0, y0 = np.array(corners[0], dtype=np.int32) ### Top left corner\n",
    "            x1, y1 = np.array(corners[2], dtype=np.int32) ### Bottom right corner\n",
    "            cv2.rectangle(mask, (x0, y0), (x1, y1), 0, -1)\n",
    "        else:\n",
    "            print(f\"Text {t.upper()} passed the filter for img index {index} and confidence {confidence[idx]}\")\n",
    "    \n",
    "    ### We now have a masked image instead of the original image. Other lang readers can now use this image.\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    ### Store the images in a new directory (masked_for_ocr/) so that I only have to do this once\n",
    "    cv2.imwrite(f\"./Data/masked_for_ocr/{index}.png\", masked_image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df = pd.read_csv('./Data/dataset/european_images.csv', index_col=0)\n",
    "masked_df['img_path'] = [f\"./Data/masked_for_ocr/{index}.png\" for index in df.index]\n",
    "masked_df['text'] =  pd.Series(dtype='object')\n",
    "masked_df['confidence'] =  pd.Series(dtype='object')\n",
    "masked_df['bbox'] =  pd.Series(dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, with the text extracted from the masked images, we can set confidence thresholds, and only keep results above specific thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is a really inefficient way to handle this but...\n",
    "Full_Results = {}\n",
    "for lang, reader in Readers.items():\n",
    "    \n",
    "    Full_Results[lang] = reader(masked_df)\n",
    "    for idx, row in Full_Results[lang].iterrows():\n",
    "        if row['text'] is not None and row['text'] != []:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass extracted text through LLM to predict country or language\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
