{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import easyocr\n",
    "from collections import Counter\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/resized_mapillary/dataset_mapillary.csv', index_col=0)\n",
    "df['img_path'] = [f\"./Data/resized_mapillary/{index}.jpg\" for index in df.index]\n",
    "df['text'] =  pd.Series(dtype='object')\n",
    "df['confidence'] =  pd.Series(dtype='object')\n",
    "#df['bbox'] =  pd.Series(dtype='object') ### For later implementations, when BBOX will be drawn over the detected text\n",
    "df['flag'] = pd.Series(0)\n",
    "df['lang'] = pd.Series(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Text Recognition\n",
    "1. It's important to note that we're looking at a plethora of European languages, which will employ various modifications to the latin script, cyrillic, etc. \n",
    "2. As such, multiple OCR readers can be employed. We can run each reader on an image, extract the confidence and various pieces of text above some threshold, and then input the collected data to our classifier.\n",
    "3. We could also use a reader with multiple languages in the list. This will remove the ability to handle different languages differently, but that may be better to avoid data leakage?\n",
    "4. It may be necessary to prune the 'Google' labels from the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jaided.ai/easyocr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ocr_reader():\n",
    "\n",
    "    def __init__(self, langs, recog_network = \"standard\"):\n",
    "        self.langs = langs\n",
    "        self.reader = easyocr.Reader(langs, detect_network = 'dbnet18', recog_network=recog_network)\n",
    "    \n",
    "    def read(self, df):\n",
    "        min_length = 5\n",
    "        self.df_results = df.copy()\n",
    "        \n",
    "        for index, row in self.df_results.iterrows():\n",
    "            \n",
    "            results = self.OCR_extraction(row['img_path'], self.reader)\n",
    "            \n",
    "            #bbox_ = []\n",
    "            text = []\n",
    "            confidence = []\n",
    "            for result in results:\n",
    "                if len(result[1]) >= min_length:\n",
    "                    self.df_results.at[index, \"flag\"] = 1\n",
    "                    print(f\"For image {index}, \")\n",
    "                    print(f\"Text is {result[1]} with confidence {result[2]}\")\n",
    "                    #bbox_.append(result[0])\n",
    "                    text.append(result[1])\n",
    "                    confidence.append(result[2])\n",
    "                \n",
    "            #self.df_results.at[index, \"bbox\"] = bbox_\n",
    "            self.df_results.at[index, \"text\"] = text\n",
    "            self.df_results.at[index, \"confidence\"] = confidence\n",
    "\n",
    "        return self.df_results\n",
    "    \n",
    "    def OCR_extraction(self, path, reader):\n",
    "        return reader.readtext(path) # , paragraph=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {\n",
    "    \"be\": \"be\",\n",
    "    \"bg\": \"bg\",\n",
    "    \"cs\": \"cs\",\n",
    "    \"cy\": \"cy\",\n",
    "    \"da\": \"da\",\n",
    "    \"de\": \"de\",\n",
    "    \"en\": \"en\",\n",
    "    \"es\": \"es\",\n",
    "    \"et\": \"et\",\n",
    "    \"fr\": \"fr\",\n",
    "    \"ga\": \"ga\",\n",
    "    \"hr\": \"hr\",\n",
    "    \"hu\": \"hu\",\n",
    "    \"is\": \"is\",\n",
    "    \"it\": \"it\",\n",
    "    \"la\": \"la\",\n",
    "    \"lt\": \"lt\",\n",
    "    \"lv\": \"lv\",\n",
    "    \"mt\": \"mt\",\n",
    "    \"nl\": \"nl\",\n",
    "    \"no\": \"no\",\n",
    "    \"pl\": \"pl\",\n",
    "    \"pt\": \"pt\",\n",
    "    \"ro\": \"ro\",\n",
    "    \"ru\": \"ru\",\n",
    "    \"rs_latin\": \"rs\",\n",
    "    \"rs_cyrillic\": \"rc\",\n",
    "    \"sk\": \"sk\",\n",
    "    \"sl\": \"sl\",\n",
    "    \"sq\": \"sq\",\n",
    "    \"sv\": \"sv\",\n",
    "    \"tr\": \"tr\",\n",
    "    \"uk\": \"uk\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_languages = [\n",
    "    \"cs\", \"cy\", \"da\", \"de\", \"en\", \"es\", \"et\", \"fr\", \"ga\", \"hr\", \"hu\", \"is\", \"it\", \"la\", \"lt\", \"lv\",\n",
    "    \"mt\", \"nl\", \"no\", \"pl\", \"pt\", \"ro\", \"rs_latin\", \"sk\", \"sl\", \"sq\", \"sv\", \"tr\"\n",
    "]\n",
    "\n",
    "cyrillic_languages = [\n",
    "    \"ru\",\"rs_cyrillic\",\"be\",\"bg\",\"uk\",\"en\"\n",
    "]\n",
    "\n",
    "Readers = {\n",
    "\"en\": ocr_reader([\"en\"]),\n",
    "\"latin\": ocr_reader(latin_languages),\n",
    "\"cyrillic\": ocr_reader(cyrillic_languages, recog_network=\"cyrillic_g2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_df = Readers[\"latin\"].read(df)\n",
    "print(\"On cyrillic\")\n",
    "cyrillic_df = Readers[\"cyrillic\"].read(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving so I don't need to run this 3 more times if I fuck something up\n",
    "latin_df.to_csv(\"latin_OCR.csv\")\n",
    "cyrillic_df.to_csv(\"cyrillic_OCR.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resume here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "latin_df = pd.read_csv(\"latin_OCR.csv\", index_col=0, converters={'text': ast.literal_eval})\n",
    "cyrillic_df = pd.read_csv(\"cyrillic_OCR.csv\", index_col=0, converters={'text': ast.literal_eval})\n",
    "latin_df['confidence'] = latin_df['confidence'].apply(ast.literal_eval)\n",
    "cyrillic_df['confidence'] = cyrillic_df['confidence'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires an anthropic key. I use a txt file that's ignored by the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import anthropic\n",
    "import os\n",
    "import time\n",
    "from itertools import islice\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "if os.path.exists(\"OCR_lang_predictions.csv\"):\n",
    "    # If the file exists, load the dataset from the file\n",
    "    df = pd.read_csv(\"OCR_lang_predictions.csv\", index_col=0)\n",
    "    \n",
    "    # Find the last valid index where \"lang_guess\" is filled\n",
    "    last_valid_idx = df[\"lang_guess\"].last_valid_index()\n",
    "    \n",
    "    if last_valid_idx is not None:\n",
    "        n = df.index.get_loc(last_valid_idx) + 1\n",
    "        print(f\"Resuming from index {n}\")\n",
    "    else:\n",
    "        n = 0\n",
    "        print(\"No valid 'lang_guess' values found. Starting from the beginning.\")\n",
    "else:\n",
    "    n = 0\n",
    "    print(\"File 'OCR_lang_predictions.csv' not found. Starting from the beginning.\")\n",
    "\n",
    "\n",
    "for idx, row in islice(latin_df.iterrows(), n, None):\n",
    "    # Constructing the JSON prompt for each row.\n",
    "    #print(row)\n",
    "    if latin_df.at[idx, \"flag\"] == 1 or cyrillic_df.at[idx, \"flag\"] == 1:\n",
    "        print(f\"Flagged index {idx}\")\n",
    "        if latin_df.at[idx, \"flag\"] == 1:\n",
    "            conf_ = latin_df.at[idx, \"confidence\"]\n",
    "            text_ = latin_df.at[idx, \"text\"]\n",
    "        else:\n",
    "            conf_ = cyrillic_df.at[idx, \"confidence\"]\n",
    "            text_ = cyrillic_df.at[idx, \"text\"]\n",
    "        \n",
    "        if latin_df.at[idx, \"flag\"] == 1 and cyrillic_df.at[idx, \"flag\"] == 1:\n",
    "            if np.max(cyrillic_df.at[idx, 'confidence']) > np.max(latin_df.at[idx, 'confidence']):\n",
    "                conf_ = cyrillic_df.at[idx, \"confidence\"]\n",
    "                text_ = cyrillic_df.at[idx, \"text\"]\n",
    "\n",
    "        # Sort text_ and conf_ based on the values in conf_\n",
    "        #print(f\"text_ is {text_}\")\n",
    "        sorted_indices = sorted(range(len(conf_)), key=lambda x: conf_[x], reverse=True)\n",
    "        sorted_text = [text_[i] for i in sorted_indices]\n",
    "        sorted_conf = [conf_[i] for i in sorted_indices]\n",
    "        #print(f\"sorted text is {sorted_text}\")\n",
    "        filtered_text = [text for text, conf in zip(sorted_text, sorted_conf) if conf >= THRESHOLD]\n",
    "        filtered_conf = [conf for conf in sorted_conf if conf >= THRESHOLD]\n",
    "\n",
    "\n",
    "        if len(filtered_text) == 0 :\n",
    "        \n",
    "            continue\n",
    "\n",
    "        # Constructing JSON query.\n",
    "        # Querying API\n",
    "        print(f\"Querying LLM for image {idx}.jpg, with peak confidence {np.max(filtered_conf)}, and filtered text {filtered_text}.\")\n",
    "        \n",
    "        input_data = {\n",
    "            \"confidence\": filtered_conf,\n",
    "            \"text\": filtered_text\n",
    "        }\n",
    "        PROMPT = json.dumps(input_data, indent=2)\n",
    "\n",
    "        parent_folder = os.path.dirname(os.path.dirname(__name__))\n",
    "\n",
    "        claude_key = open(parent_folder + \"claude_api_key.txt\", \"r\").read()\n",
    "\n",
    "        client = anthropic.Anthropic(\n",
    "            api_key=claude_key,\n",
    "            max_retries=2,  \n",
    "            timeout=60.0    \n",
    "        )\n",
    "\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                max_tokens=1000,\n",
    "                temperature=0,\n",
    "                system=\"You are tasked with determining the language of a string, or set of strings. The strings were extracted from randomly sampled images in a dataset. The strings may or may not be meaningless.\\n\\nThe input will be formatted as a JSON input. There will be two entries - \\n1) A list of confidence metrics, ranging from 0 to 1, which is output from an EasyOCR text extraction model. One confidence metric for each string.\\n2) A list of strings, which were extracted from the model.\\n\\nInput format is of the form:\\n{\\n  \\\"confidence\\\": [0.5, 0.3],\\n  \\\"text\\\": [\\\"Extracted text 1\\\", \\\"Extracted text 2\\\"]\\n}\\n\\nNaturally, the confidence at index i corresponds to the string at index i.\\n\\nPlease output two values:\\n\\n1) An integer confidence between 0 and 10. This does not need to be precise - I'd just like to differentiate between \\\"very confident\\\" and \\\"not at all confident\\\". As a model, you are effectively performing two functions: a) filtering out meaningless text OCR (for example, a response like \\\"Kf8xc\\\"), and b) identifying the language if and only if you're very confident.\\n2) A language guess, using the 2-character standard identifier, e.g. \\\"en\\\" or \\\"de\\\"\\n\\n\\nYour output should be of the form:\\n\\n{\\n  \\\"confidence\\\": 3,\\n  \\\"lang\\\": \\\"en\\\"\\n}\\n\\nDo not include any additional information in your response - only return the JSON output as described above. \\n\\nIf multiple languages seem present (for instance, both Serbian (Latin) and Serbian (Cyrillic), I would like you to return both languages, sorted by the confidence, as lists. For instance:\\n\\n{\\n  \\\"confidence\\\": [0.7, 0.4],\\n  \\\"lang\\\": [\\\"en\\\", \\\"de\\\"]\\n}\\n\\nIf a language has no matches (for instance, if the language is Mongolian or Chinese, which are not in the list of target languages), the confidence should be set to 0. \\nIf all of the text is numeric (as would be expected from e.g. speed limit signs) the confidence should be low. \\nIf the text seems like the metadata of a camera (for instance, data/time info from a dash-cam), confidence should be set very low, as the text is unrelated to text within the background. \\nIf the text appears to have small typos (e.g. \\\"Rrestaurant\\\"), you may lower the confidence, but assume that this is due to variations in the font, or errors in the OCR model.\\nIf the text is the name of a city, retail store, or other regional entity, (for instance, \\\"Wrocław\\\"), please return the dominant language spoken in that region (in this case, \\\"pl\\\").\\n\\n\\nThe languages that you may choose from are as follows:\\n\\n{\\n\\\"Belarusian\\\": \\\"be\\\",\\n\\\"Bulgarian\\\": \\\"bg\\\",\\n\\\"Czech\\\": \\\"cs\\\",\\n\\\"Welsh\\\": \\\"cy\\\",\\n\\\"Danish\\\": \\\"da\\\",\\n\\\"German\\\": \\\"de\\\",\\n\\\"English\\\": \\\"en\\\",\\n\\\"Spanish\\\": \\\"es\\\",\\n\\\"Estonian\\\": \\\"et\\\",\\n\\\"French\\\": \\\"fr\\\",\\n\\\"Irish\\\": \\\"ga\\\",\\n\\\"Croatian\\\": \\\"hr\\\",\\n\\\"Hungarian\\\": \\\"hu\\\",\\n\\\"Icelandic\\\": \\\"is\\\",\\n\\\"Italian\\\": \\\"it\\\",\\n\\\"Latin\\\": \\\"la\\\",\\n\\\"Lithuanian\\\": \\\"lt\\\",\\n\\\"Latvian\\\": \\\"lv\\\",\\n\\\"Maltese\\\": \\\"mt\\\",\\n\\\"Dutch\\\": \\\"nl\\\",\\n\\\"Norwegian\\\": \\\"no\\\",\\n\\\"Polish\\\": \\\"pl\\\",\\n\\\"Portuguese\\\": \\\"pt\\\",\\n\\\"Romanian\\\": \\\"ro\\\",\\n\\\"Russian\\\": \\\"ru\\\",\\n\\\"Serbian (Latin)\\\": \\\"rs\\\",\\n\\\"Serbian (Cyrillic)\\\": \\\"rc\\\",\\n\\\"Slovak\\\": \\\"sk\\\",\\n\\\"Slovenian\\\": \\\"sl\\\",\\n\\\"Albanian\\\": \\\"sq\\\",\\n\\\"Swedish\\\": \\\"sv\\\",\\n\\\"Turkish\\\": \\\"tr\\\",\\n\\\"Ukrainian\\\": \\\"uk\\\"\\n}\\n\\nIf the input is invalid, please return instead error 4xx, and a description of why the input is invalid.\\n\\n{\\n \\\"response\\\": 4xx,\\n \\\"error\\\": \\\"The issue with the input\\\"\\n}\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": PROMPT\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        except anthropic.APIConnectionError as e:\n",
    "            print(f\"The server could not be reached for image {idx}.jpg\")\n",
    "            print(f\"Underlying exception: {e.__cause__}\")\n",
    "            break\n",
    "        except anthropic.RateLimitError as e:\n",
    "            print(f\"A 429 status code was received for image {idx}.jpg; backing off\")\n",
    "            break\n",
    "            \n",
    "        except anthropic.APIStatusError as e:\n",
    "            print(f\"A non-200 status code was received for image {idx}.jpg\")\n",
    "            print(f\"Status code: {e.status_code}\")\n",
    "            print(f\"Response: {e.response}\")\n",
    "            break\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON response for image {idx}.jpg: {message.content}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            OUTPUT = json.loads(message.content[0].text)\n",
    "        except:\n",
    "            print(f\"Some error with {idx}, giving output {message.content[0].text}. Skipping\")\n",
    "            break\n",
    "\n",
    "        language_LLM = OUTPUT.get(\"lang\")\n",
    "        confidence_LLM = OUTPUT.get(\"confidence\")\n",
    "\n",
    "        if language_LLM is not None and confidence_LLM is not None:\n",
    "            print(f\"For image {idx}, detected language: {language_LLM}, LLM confidence score: {confidence_LLM}\")\n",
    "        else:\n",
    "            print(\"Missing 'lang' or 'confidence' key in the JSON output.\")\n",
    "            \n",
    "        # Including guess with confidence from LLM\n",
    "        # Scaling from 1:10 to 0.1:1\n",
    "        if isinstance(confidence_LLM, list):\n",
    "            confidence_LLM = confidence_LLM[0]\n",
    "            language_LLM = language_LLM[0]\n",
    "            ### Flagging so I know there's multiple guesses\n",
    "            df.at[idx, \"flag\"] = 1\n",
    "        df.at[idx, \"lang_guess\"] = language_LLM\n",
    "        df.at[idx, \"conf\"] = confidence_LLM \n",
    "        df.to_csv(\"OCR_lang_predictions.csv\")\n",
    "\n",
    "\n",
    "        time.sleep(30) # Waiting, so that I don't burn $500 instantly if this is more expensive than expected\n",
    "        # I suppose it's not necessary since the model takes upwards of 10 seconds anyways. But, w/e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate a file containing the index and the OCR model confidence.\n",
    "#### Pass this into a VLLM (Claude Opus or GPT-4 right now)\n",
    "#### VLLM will output a language guess and a confidence score between 0:10.\n",
    "#### Drop all guesses below threshold c.\n",
    "\n",
    "import anthropic\n",
    "import os\n",
    "parent_folder = os.path.dirname(os.path.dirname(__name__))\n",
    "#google_api_key = open(parent_folder + \"google_maps_api_key.txt\", \"r\").read()\n",
    "claude_key = open(parent_folder + \"claude_api_key.txt\", \"r\").read()\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=claude_key\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=\"You are tasked with determining the language of a string, or set of strings. The strings were extracted from randomly sampled images in a dataset. The strings may or may not be meaningless.\\n\\nThe input will be formatted as a JSON input. There will be two entries - \\n1) A list of confidence metrics, ranging from 0 to 1, which is output from an EasyOCR text extraction model. One confidence metric for each string.\\n2) A list of strings, which were extracted from the model.\\n\\nInput format is of the form:\\n{\\n  \\\"confidence\\\": [0.5, 0.3],\\n  \\\"text\\\": [\\\"Extracted text 1\\\", \\\"Extracted text 2\\\"]\\n}\\n\\nNaturally, the confidence at index i corresponds to the string at index i.\\n\\nPlease output two values:\\n\\n1) An integer confidence between 0 and 10. This does not need to be precise - I'd just like to differentiate between \\\"very confident\\\" and \\\"not at all confident\\\". As a model, you are effectively performing two functions: a) filtering out meaningless text OCR (for example, a response like \\\"Kf8xc\\\"), and b) identifying the language if and only if you're very confident.\\n2) A language guess, using the 2-character standard identifier, e.g. \\\"en\\\" or \\\"de\\\"\\n\\n\\nYour output should be of the form:\\n\\n{\\n  \\\"confidence\\\": 3,\\n  \\\"lang\\\": \\\"en\\\"\\n}\\n\\nDo not include any additional information in your response - only return the JSON output as described above. \\n\\nIf multiple languages seem present (for instance, both Serbian (Latin) and Serbian (Cyrillic), I would like you to return both languages, sorted by the confidence, as lists. For instance:\\n\\n{\\n  \\\"confidence\\\": [0.7, 0.4],\\n  \\\"lang\\\": [\\\"en\\\", \\\"de\\\"]\\n}\\n\\nIf a language has no matches (for instance, if the language is Mongolian or Chinese, which are not in the list of target languages), the confidence should be set to 0. \\nIf all of the text is numeric (as would be expected from e.g. speed limit signs) the confidence should be low. \\nIf the text seems like the metadata of a camera (for instance, data/time info from a dash-cam), confidence should be set very low, as the text is unrelated to text within the background. \\nIf the text appears to have small typos (e.g. \\\"Rrestaurant\\\"), you may lower the confidence, but assume that this is due to variations in the font, or errors in the OCR model.\\nIf the text is the name of a city, retail store, or other regional entity, (for instance, \\\"Wrocław\\\"), please return the dominant language spoken in that region (in this case, \\\"pl\\\").\\n\\n\\nThe languages that you may choose from are as follows:\\n\\n{\\n\\\"Belarusian\\\": \\\"be\\\",\\n\\\"Bulgarian\\\": \\\"bg\\\",\\n\\\"Czech\\\": \\\"cs\\\",\\n\\\"Welsh\\\": \\\"cy\\\",\\n\\\"Danish\\\": \\\"da\\\",\\n\\\"German\\\": \\\"de\\\",\\n\\\"English\\\": \\\"en\\\",\\n\\\"Spanish\\\": \\\"es\\\",\\n\\\"Estonian\\\": \\\"et\\\",\\n\\\"French\\\": \\\"fr\\\",\\n\\\"Irish\\\": \\\"ga\\\",\\n\\\"Croatian\\\": \\\"hr\\\",\\n\\\"Hungarian\\\": \\\"hu\\\",\\n\\\"Icelandic\\\": \\\"is\\\",\\n\\\"Italian\\\": \\\"it\\\",\\n\\\"Latin\\\": \\\"la\\\",\\n\\\"Lithuanian\\\": \\\"lt\\\",\\n\\\"Latvian\\\": \\\"lv\\\",\\n\\\"Maltese\\\": \\\"mt\\\",\\n\\\"Dutch\\\": \\\"nl\\\",\\n\\\"Norwegian\\\": \\\"no\\\",\\n\\\"Polish\\\": \\\"pl\\\",\\n\\\"Portuguese\\\": \\\"pt\\\",\\n\\\"Romanian\\\": \\\"ro\\\",\\n\\\"Russian\\\": \\\"ru\\\",\\n\\\"Serbian (Latin)\\\": \\\"rs\\\",\\n\\\"Serbian (Cyrillic)\\\": \\\"rc\\\",\\n\\\"Slovak\\\": \\\"sk\\\",\\n\\\"Slovenian\\\": \\\"sl\\\",\\n\\\"Albanian\\\": \\\"sq\\\",\\n\\\"Swedish\\\": \\\"sv\\\",\\n\\\"Turkish\\\": \\\"tr\\\",\\n\\\"Ukrainian\\\": \\\"uk\\\"\\n}\\n\\nIf the input is invalid, please return instead error 4xx, and a description of why the input is invalid.\\n\\n{\\n \\\"response\\\": 4xx,\\n \\\"error\\\": \\\"The issue with the input\\\"\\n}\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"{\\n  \\\"confidence\\\": [0.999],\\n  \\\"text\\\": [\\\"MILGIAI\\\"]\\n}\\n\\n\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything below is unused, since the Mapillary data does not have watermarks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~Strip Google watermarks from detected text~~\n",
    "### This is not necessary for the Mapillary dataset, but I'm leaving the algorithm in, since it may be useful to other devs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text for each image for each reader, then using manual pruning to determine which labels to drop for each language.\n",
    "These can then be associated with the correct language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the english reader, identify matching watermark text, determine the bounding box, and mask the image for that watermark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OCR_extraction(path, reader):\n",
    "    results =  reader.readtext(path)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.makedirs(\"./Data/masked_for_ocr/\", exist_ok=True)\n",
    "\n",
    "reader_en = Readers[\"en\"].reader\n",
    "\n",
    "watermarks = [\"gccole\", \"cgoogle\", \"cgccgle\", \"cgcogle\", \"cgocgle\", \"google\", \"gccgle\", \"gcogle\", \"gocgle\", \"gocnle\", \"gcogie\", \"oogle\"]\n",
    "\n",
    "### For each image, generate a mask (initially blank). Use this to mask results for the other readers.\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ### Generating the mask with the english reader, filtering out the sections of the image containing the \"Google\" watermark.\n",
    "    image = cv2.imread(row['img_path'])\n",
    "    mask = np.ones(image.shape[:2], dtype=np.uint8) * 255\n",
    "    results = OCR_extraction(row['img_path'], reader_en)\n",
    "    \n",
    "    bbox_ = []\n",
    "    text = []\n",
    "    confidence = []\n",
    "    for result in results:\n",
    "        bbox_.append(result[0])\n",
    "        text.append(result[1])\n",
    "        confidence.append(result[2])\n",
    "    \n",
    "    for idx, t in enumerate(text):\n",
    "        ### Filtereing text matching the filter, or text with less than min_length characters (meaningless)\n",
    "        min_length = 3\n",
    "        if any(watermark.upper() in t.upper() for watermark in watermarks) or len(t) <= min_length:\n",
    "            corners = bbox_[idx]\n",
    "            x0, y0 = np.array(corners[0], dtype=np.int32) ### Top left corner\n",
    "            x1, y1 = np.array(corners[2], dtype=np.int32) ### Bottom right corner\n",
    "            cv2.rectangle(mask, (x0, y0), (x1, y1), 0, -1)\n",
    "        else:\n",
    "            print(f\"Text {t.upper()} passed the filter for img index {index} and confidence {confidence[idx]}\")\n",
    "    \n",
    "    ### We now have a masked image instead of the original image. Other lang readers can now use this image.\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    ### Store the images in a new directory (masked_for_ocr/) so that I only have to do this once\n",
    "    cv2.imwrite(f\"./Data/masked_for_ocr/{index}.png\", masked_image)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df = pd.read_csv('./Data/dataset/european_images.csv', index_col=0)\n",
    "masked_df['img_path'] = [f\"./Data/masked_for_ocr/{index}.png\" for index in df.index]\n",
    "masked_df['text'] =  pd.Series(dtype='object')\n",
    "masked_df['confidence'] =  pd.Series(dtype='object')\n",
    "masked_df['bbox'] =  pd.Series(dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, with the text extracted from the masked images, we can set confidence thresholds, and only keep results above specific thresholds.\n",
    "\n",
    "##### Identify the language with the highest confidence, and use this as our guessed label\n",
    "##### Additionally, we can extract text and pass forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the extracted text into an LLM. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
